# -*- coding: utf-8 -*-
__author__ = 'duyongan'
__date__ = '2018/6/28 10:01'
import jieba
import re
import jieba.posseg as pseg
import networkx as nx
from pylab import *
mpl.rcParams['font.sans-serif'] = ['SimHei']
import numpy as np
import collections
import nltk

class analyse:
    def __init__(self,text,num_of_keywords,num_of_abstract,stopwords,lang,idf_map):
        self.text = text
        self.num_of_keywords = num_of_keywords
        self.num_of_abstract = num_of_abstract
        self.stopwords = stopwords
        self.lang = lang
        self.idf_map = idf_map
        text = text.replace('\n', '').replace('\u3000', '').replace('？”', '”').replace('！”', '”').replace('。”', '”')
        sentences = re.split(r"([。！？…….?!])", text)
        sentences.append('')
        self.sentences = ["".join(i) for i in zip(sentences[0::2], sentences[1::2])]
        if lang=='zh':
            # 分词
            sentences2=[]
            for sentence in self.sentences:
                words =list(pseg.cut(sentence))
                words2=[]
                temp=''
                for i in range(len(words)):
                    if words[i].flag in ['n','nd','nh','ni','nl','ns','nt','nz','vn','nr','nrf','nsf','ng','nrj','nr1','nr2'] and len(temp)<=4:
                        if words[i].word not in stopwords:
                            temp=temp+words[i].word
                        if i==len(words)-1:
                            if temp.strip()!='':
                                words2.append(temp)
                    else:
                        if temp.strip()!='':
                            words2.append(temp)
                        temp=''
                        if words[i].flag=='eng':
                            en_word=nltk.pos_tag([words[i].word])[0][1]
                            if  en_word in ['NN','NNS','NNP','NNPS']:
                                words2.append(words[i].word)
                sentences2.append(words2)
        elif lang=='en':
            sentences2=[]
            for sentence in self.sentences:
                words =list(nltk.pos_tag(sentence.split()))
                words2=[]
                temp=''
                for i in range(len(words)):
                    if words[i][1] in ['NN','NNS','NNP','NNPS'] and len(temp)<=4:
                        temp=temp+words[i][0]
                        if i==len(words)-1:
                            if temp.strip()!='':
                                words2.append(temp)
                    else:
                        if temp.strip()!='':
                            words2.append(temp)
                        temp=''
                sentences2.append(words2)
        else:
            return []
        #去停用词和单字
        self.sentences3=[]
        for sentence in sentences2:
            sentence2=[]
            for word in sentence:
                if word in stopwords:
                    pass
                elif len(word)<=2:
                    pass
                else:
                    sentence2.append(word)
            if len(sentence2)>1:
                self.sentences3.append(sentence2)

        #单字词频统计
        def flatten(x):
            result = []
            for el in x:
                if isinstance(x, collections.Iterable) and not isinstance(el, str):
                    result.extend(flatten(el))
                else:
                    result.append(el)
            return result
        word_map={}
        word_list=flatten(self.sentences3)
        for word in list(set(word_list)):
            word_map[word]=word_list.count(word)
        #词对频数统计
        word2word_map={}
        word2word=[]
        for sentence in self.sentences3:
            for i in range(len(sentence)):
                for j in range(i+1,len(sentence)):
                    alist=[]
                    alist.append(sentence[i])
                    alist.append(sentence[j])
                    alist=sorted(alist)
                    word2word.append('_'.join(alist))
        for w2w in list(set(word2word)):
            word2word_map[w2w]=word2word.count(w2w)

        #计算共现网络权重
        word2word_weight={}
        for w2w in list(set(word2word)):
            word2word_weight[w2w]=(word2word_map[w2w]/word_map[w2w.split('_')[0]]+word2word_map[w2w]/word_map[w2w.split('_')[1]])*0.5

        #共现网络可视化
        G=nx.Graph()
        word_list2=[]
        for word in list(set(word_list)):
            word=unicode(word.encode("utf-8"),'utf-8')
            word_list2.append(word)
        G.add_nodes_from(word_list2)
        for w2w in word2word_weight.keys():
            G.add_edge(unicode(w2w.split('_')[0].encode("utf-8"),'utf-8'),unicode(w2w.split('_')[1].encode("utf-8"),'utf-8'),weight=word2word_weight[w2w])
        word_map2=[]
        for word in nx.degree_centrality(G).keys():
            word_map2.append((word,word_map[word]))
        fre=[]
        idf_num=[]
        max_idf_num=max(idf_map.values())
        for flu in word_map2:
            fre.append(flu[1])
            try:
                idf_num.append(idf_map[flu[0]])
            except:
                idf_num.append(max_idf_num)
        fre=np.array(fre)
        idf_num=np.array(idf_num)
        try:
            pr1=np.array(list(nx.degree_centrality(G).values()))
        except:
            pr1=np.zeros(len(word_list2))
        try:
            pr2=np.array(list(nx.eigenvector_centrality(G).values()))
        except:
            pr2=np.zeros(len(word_list2))
        try:
            pr3=np.array(list(nx.betweenness_centrality(G).values()))
        except:
            pr3=np.zeros(len(word_list2))
        pr4=fre
        weight_=list((0.1*pr1/max(0.001,sum(pr1))+0.1*pr2/max(0.001,sum(pr2))+0.5*pr3/max(0.001,sum(pr3))+0.3*pr4*idf_num/max(0.001,sum(pr4))))
        self.keywords=dict(zip(word_list2,weight_))


    def getKeywords(self):
        keywords = sorted(self.keywords.items(), key=lambda k: k[1])
        keywords_dup = ''
        i = 1
        j = 1
        result_ = []
        while j < self.num_of_keywords + 1:
            try:
                wor = ''.join(sorted(list(jieba.cut(keywords[-i][0]))))
                if keywords_dup.find(wor) != -1:
                    pass
                else:
                    keywords_dup = keywords_dup + wor
                    result_.append(keywords[-i][0])
                    j += 1
                i += 1
            except:
                break
        return result_


    def getAbstract(self):
        sentences_score = {}
        i = 0
        for sentence in self.sentences3:
            sentence_score = 0
            if len(sentence) > 0:
                for word in sentence:
                    sentence_score += self.keywords[word]
                sentences_score[i] = sentence_score / len(sentence)
            i += 1
        sentences_score = sorted(sentences_score.items(), reverse=True, key=lambda k: k[1])
        results = []
        for sentence_num in sorted(sentences_score[:self.num_of_abstract]):
            sentence = ''
            seq = re.split('([，；])', self.sentences[sentence_num[0]])
            seq.append('')
            seq = ["".join(i) for i in zip(seq[0::2], seq[1::2])]
            for sen in seq:
                words = list(jieba.cut(sen))
                words2 = []
                for word in words:
                    if word in self.stopwords:
                        pass
                    elif len(word) <= 1:
                        pass
                    else:
                        words2.append(word)
                if len(sen) <= 4:
                    __if_useful = False
                    word_flags = pseg.lcut(sen)
                    for word_flag in word_flags:
                        if word_flag.flag in ['n', 'nd', 'nh', 'ni', 'nl', 'ns', 'nt', 'nz', 'vn', 'nr', 'nrf', 'nsf',
                                              'ng', 'nrj', 'nr1', 'nr2']:
                            __if_useful = True
                            break
                    if __if_useful:
                        if sentence != '':
                            sentence = sentence + sen
                elif len(words2) == 0:
                    pass
                else:
                    if sentence != '':
                        sentence = sentence + sen
                    else:
                        sentence = sentence + sen
            results.append(sentence)
        return ''.join(results)
